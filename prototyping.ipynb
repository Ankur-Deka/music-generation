{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mido\n",
    "import string\n",
    "from music21 import midi\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg2dict(msg):\n",
    "    result = dict()\n",
    "    if 'note_on' in msg:\n",
    "        on_ = True\n",
    "    elif 'note_off' in msg:\n",
    "        on_ = False\n",
    "    else:\n",
    "        on_ = None\n",
    "    result['time'] = int(msg[msg.rfind('time'):].split(' ')[0].split('=')[1].translate(\n",
    "        str.maketrans({a: None for a in string.punctuation})))\n",
    "\n",
    "    if on_ is not None:\n",
    "        for k in ['note', 'velocity']:\n",
    "            result[k] = int(msg[msg.rfind(k):].split(' ')[0].split('=')[1].translate(\n",
    "                str.maketrans({a: None for a in string.punctuation})))\n",
    "    return [result, on_]\n",
    "\n",
    "def switch_note(last_state, note, velocity, on_=True):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of this range will be ignored\n",
    "    result = [0] * 88 if last_state is None else last_state.copy()\n",
    "    if 21 <= note <= 108:\n",
    "        result[note-21] = velocity if on_ else 0\n",
    "    return result\n",
    "\n",
    "def get_new_state(new_msg, last_state):\n",
    "    new_msg, on_ = msg2dict(str(new_msg))\n",
    "    new_state = switch_note(last_state, note=new_msg['note'], velocity=new_msg['velocity'], on_=on_)\\\n",
    "                if on_ is not None else last_state\n",
    "    return [new_state, new_msg['time']]\n",
    "\n",
    "def track2seq(track):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of the id range will be ignored\n",
    "    result = []\n",
    "    last_state, last_time = get_new_state(str(track[0]), [0]*88)\n",
    "    for i in range(1, len(track)):\n",
    "        new_state, new_time = get_new_state(track[i], last_state)\n",
    "        if new_time > 0:\n",
    "            result += [last_state]*new_time\n",
    "        last_state, last_time = new_state, new_time\n",
    "    return result\n",
    "\n",
    "def mid2arry(mid, min_msg_pct=0.1):\n",
    "    tracks_len = [len(tr) for tr in mid.tracks]\n",
    "    min_n_msg = max(tracks_len) * min_msg_pct\n",
    "    # convert each track to nested list\n",
    "    all_arys = []\n",
    "    for i in range(len(mid.tracks)):\n",
    "        if len(mid.tracks[i]) > min_n_msg:\n",
    "            ary_i = track2seq(mid.tracks[i])\n",
    "            all_arys.append(ary_i)\n",
    "    # make all nested list the same length\n",
    "    max_len = max([len(ary) for ary in all_arys])\n",
    "    for i in range(len(all_arys)):\n",
    "        if len(all_arys[i]) < max_len:\n",
    "            all_arys[i] += [[0] * 88] * (max_len - len(all_arys[i])) # adding 0's at the end\n",
    "                            \n",
    "    final_arr = np.array(all_arys, dtype=np.int8)\n",
    "    final_arr = final_arr.max(axis=0)\n",
    "    # trim: remove consecutive 0s in the beginning and at the end\n",
    "    sums = final_arr.sum(axis=1)\n",
    "    ends = np.where(sums > 0)[0]\n",
    "    final_arr = final_arr[min(ends): max(ends)]\n",
    "    return final_arr\n",
    "\n",
    "\n",
    "# extracts overlapping 10sec clips from single midi_file\n",
    "def extract_clips(file, idx, store_path, counter = 0, ticks = 10000, overlap=False):\n",
    "    # load the file\n",
    "    mid = mido.MidiFile(file)\n",
    "    \n",
    "    # extract array\n",
    "    array = mid2arry(mid)\n",
    "    array2 = array[ticks//2:] # to get 50% overlap\n",
    "    \n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    \n",
    "    i = counter\n",
    "    arrays = [array,array2] if overlap else [array] \n",
    "    for arr in arrays:\n",
    "        n = (arr.shape[0]//ticks)*ticks\n",
    "        clips = np.array_split(arr, np.arange(ticks, n, ticks))\n",
    "        for c in clips[:-1]:\n",
    "            np.savez_compressed('{}/{}'.format(store_path,i), clip=c[::500], artist_idx=idx)\n",
    "            i += 1\n",
    "    return i\n",
    "    \n",
    "def preprocess(data_dir, store_path, min_clips=100):\n",
    "    print('Preprocessing data')\n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    artists_list  = np.sort(os.listdir(data_dir))\n",
    "    data_path = os.path.join(store_path, 'data')\n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'w') as f:\n",
    "        f.write(str(artists_list))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.arange(artists_list.shape[0])))\n",
    "    counter = 0\n",
    "    counts_string = '\\n'\n",
    "    for idx,artist in enumerate(artists_list):\n",
    "        path = os.path.join(data_dir, artist)\n",
    "        files = os.listdir(path)\n",
    "        midi_files = []\n",
    "        for f in files:\n",
    "            if f.endswith('mid'):\n",
    "                midi_files.append(f)\n",
    "        midi_files.sort()\n",
    "        prev_counter = counter\n",
    "        for file in midi_files:\n",
    "            counter = extract_clips(os.path.join(path, file),idx, data_path, counter)\n",
    "            if counter-prev_counter>=min_clips:\n",
    "                break\n",
    "        counts_string += '{}: {}\\n'.format(artist, counter-prev_counter)\n",
    "        print(artist, counter-prev_counter)\n",
    "    \n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'a') as f:\n",
    "        f.write(counts_string)\n",
    "\n",
    "# stores files from each artist separately (useful for seq2seq, generation)\n",
    "def preprocess_separate(data_dir, store_path, min_clips=1000):\n",
    "    print('Preprocessing data')\n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    artists_list  = np.sort(os.listdir(data_dir))\n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'w') as f:\n",
    "        f.write(str(artists_list))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.arange(artists_list.shape[0])))\n",
    "    counts_string = '\\n'\n",
    "    for idx,artist in enumerate(artists_list):\n",
    "        path = os.path.join(data_dir, artist)\n",
    "        files = os.listdir(path)\n",
    "        midi_files = []\n",
    "        for f in files:\n",
    "            if f.endswith('mid'):\n",
    "                midi_files.append(f)\n",
    "        midi_files.sort()\n",
    "        counter = 0\n",
    "        for file in midi_files:\n",
    "            counter = extract_clips(os.path.join(path, file),idx, os.path.join(store_path, artist), counter)\n",
    "            if counter>=min_clips:\n",
    "                break\n",
    "        counts_string += '{}: {}\\n'.format(artist, counter)\n",
    "        print(artist, counter)\n",
    "        break\n",
    "    \n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'a') as f:\n",
    "        f.write(counts_string)\n",
    "\n",
    "# preprocess('./dataset', './preprocessed_dataset')\n",
    "preprocess_separate('./dataset', './preprocessed_separate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small dataset\n",
    "## Extract notes of bach and bartok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# split into test and train\n",
    "in_dir = 'dataset'\n",
    "out_dir = 'small_dataset'\n",
    "test_frac = 0.2\n",
    "# first split the dataset\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "# artists_list  = np.sort(os.listdir(data_dir))\n",
    "artists_list = ['bach', 'bartok']\n",
    "\n",
    "def create_copies(in_path, out_path, files):\n",
    "    for f in files:\n",
    "        shutil.copyfile(os.path.join(in_path, f), os.path.join(out_path, f))\n",
    "for artist in artists_list:\n",
    "    files = os.listdir(os.path.join(in_dir, artist))\n",
    "    files.sort()\n",
    "    n = len(files)\n",
    "    n_test = int(test_frac*n)\n",
    "    n_train = n - n_test\n",
    "    \n",
    "    if not os.path.exists(os.path.join(out_dir, artist,'train')):\n",
    "        os.makedirs(os.path.join(out_dir, artist, 'train'))\n",
    "        os.makedirs(os.path.join(out_dir, artist, 'test'))\n",
    "    \n",
    "    train_files = files[:n_train]\n",
    "    test_files = files[n_train:]\n",
    "    \n",
    "    create_copies(os.path.join(in_dir, artist), os.path.join(out_dir, artist, 'train'), train_files)\n",
    "    create_copies(os.path.join(in_dir, artist), os.path.join(out_dir, artist, 'test'), test_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract notes for each and get total unique notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing small_dataset/bach/train/bach354.mid\n",
      "Parsing small_dataset/bach/train/bach343.mid\n",
      "Parsing small_dataset/bach/train/bach356.mid\n",
      "Parsing small_dataset/bach/train/bach342.mid\n",
      "Parsing small_dataset/bach/train/bach370.mid\n",
      "Parsing small_dataset/bach/train/bach350.mid\n",
      "Parsing small_dataset/bach/train/bach359.mid\n",
      "Parsing small_dataset/bach/train/bach353.mid\n",
      "Parsing small_dataset/bach/train/bach361.mid\n",
      "Parsing small_dataset/bach/train/bach373.mid\n",
      "Parsing small_dataset/bach/train/bach345.mid\n",
      "Parsing small_dataset/bach/train/bach367.mid\n",
      "Parsing small_dataset/bach/train/bach360.mid\n",
      "Parsing small_dataset/bach/train/bach349.mid\n",
      "Parsing small_dataset/bach/train/bach368.mid\n",
      "Parsing small_dataset/bach/train/bach347.mid\n",
      "Parsing small_dataset/bach/train/bach351.mid\n",
      "Parsing small_dataset/bach/train/bach365.mid\n",
      "Parsing small_dataset/bach/train/bach358.mid\n",
      "Parsing small_dataset/bach/train/bach376.mid\n",
      "Parsing small_dataset/bach/train/bach372.mid\n",
      "Parsing small_dataset/bach/train/bach369.mid\n",
      "Parsing small_dataset/bach/train/bach363.mid\n",
      "Parsing small_dataset/bach/train/bach377.mid\n",
      "Parsing small_dataset/bach/train/bach346.mid\n",
      "Parsing small_dataset/bach/train/bach344.mid\n",
      "Parsing small_dataset/bach/train/bach375.mid\n",
      "Parsing small_dataset/bach/train/bach381.mid\n",
      "Parsing small_dataset/bach/train/bach371.mid\n",
      "Parsing small_dataset/bach/train/bach378.mid\n",
      "Parsing small_dataset/bach/train/bach352.mid\n",
      "Parsing small_dataset/bach/train/bach357.mid\n",
      "Parsing small_dataset/bach/train/bach348.mid\n",
      "Parsing small_dataset/bach/train/bach366.mid\n",
      "Parsing small_dataset/bach/train/bach380.mid\n",
      "Parsing small_dataset/bach/train/bach355.mid\n",
      "Parsing small_dataset/bach/train/bach374.mid\n",
      "Parsing small_dataset/bach/train/bach379.mid\n",
      "Parsing small_dataset/bach/train/bach362.mid\n",
      "Parsing small_dataset/bach/train/bach364.mid\n",
      "Parsing small_dataset/bach/test/bach387.mid\n",
      "Parsing small_dataset/bach/test/bach388.mid\n",
      "Parsing small_dataset/bach/test/bach384.mid\n",
      "Parsing small_dataset/bach/test/bach383.mid\n",
      "Parsing small_dataset/bach/test/bach389.mid\n",
      "Parsing small_dataset/bach/test/bach382.mid\n",
      "Parsing small_dataset/bach/test/bach385.mid\n",
      "Parsing small_dataset/bach/test/bach390.mid\n",
      "Parsing small_dataset/bach/test/bach386.mid\n",
      "Parsing small_dataset/bach/test/bach391.mid\n",
      "Parsing small_dataset/bartok/train/bartok417.mid\n",
      "Parsing small_dataset/bartok/train/bartok429.mid\n",
      "Parsing small_dataset/bartok/train/bartok427.mid\n",
      "Parsing small_dataset/bartok/train/bartok418.mid\n",
      "Parsing small_dataset/bartok/train/bartok428.mid\n",
      "Parsing small_dataset/bartok/train/bartok423.mid\n",
      "Parsing small_dataset/bartok/train/bartok399.mid\n",
      "Parsing small_dataset/bartok/train/bartok405.mid\n",
      "Parsing small_dataset/bartok/train/bartok406.mid\n",
      "Parsing small_dataset/bartok/train/bartok396.mid\n",
      "Parsing small_dataset/bartok/train/bartok407.mid\n",
      "Parsing small_dataset/bartok/train/bartok426.mid\n",
      "Parsing small_dataset/bartok/train/bartok402.mid\n",
      "Parsing small_dataset/bartok/train/bartok410.mid\n",
      "Parsing small_dataset/bartok/train/bartok401.mid\n",
      "Parsing small_dataset/bartok/train/bartok425.mid\n",
      "Parsing small_dataset/bartok/train/bartok422.mid\n",
      "Parsing small_dataset/bartok/train/bartok414.mid\n",
      "Parsing small_dataset/bartok/train/bartok419.mid\n",
      "Parsing small_dataset/bartok/train/bartok397.mid\n",
      "Parsing small_dataset/bartok/train/bartok394.mid\n",
      "Parsing small_dataset/bartok/train/bartok409.mid\n",
      "Parsing small_dataset/bartok/train/bartok395.mid\n",
      "Parsing small_dataset/bartok/train/bartok424.mid\n",
      "Parsing small_dataset/bartok/train/bartok392.mid\n",
      "Parsing small_dataset/bartok/train/bartok420.mid\n",
      "Parsing small_dataset/bartok/train/bartok430.mid\n",
      "Parsing small_dataset/bartok/train/bartok398.mid\n",
      "Parsing small_dataset/bartok/train/bartok411.mid\n",
      "Parsing small_dataset/bartok/train/bartok400.mid\n",
      "Parsing small_dataset/bartok/train/bartok403.mid\n",
      "Parsing small_dataset/bartok/train/bartok393.mid\n",
      "Parsing small_dataset/bartok/train/bartok408.mid\n",
      "Parsing small_dataset/bartok/train/bartok412.mid\n",
      "Parsing small_dataset/bartok/train/bartok415.mid\n",
      "Parsing small_dataset/bartok/train/bartok404.mid\n",
      "Parsing small_dataset/bartok/train/bartok413.mid\n",
      "Parsing small_dataset/bartok/train/bartok416.mid\n",
      "Parsing small_dataset/bartok/train/bartok421.mid\n",
      "Parsing small_dataset/bartok/test/bartok432.mid\n",
      "Parsing small_dataset/bartok/test/bartok440.mid\n",
      "Parsing small_dataset/bartok/test/bartok438.mid\n",
      "Parsing small_dataset/bartok/test/bartok437.mid\n",
      "Parsing small_dataset/bartok/test/bartok431.mid\n",
      "Parsing small_dataset/bartok/test/bartok439.mid\n",
      "Parsing small_dataset/bartok/test/bartok433.mid\n",
      "Parsing small_dataset/bartok/test/bartok435.mid\n",
      "Parsing small_dataset/bartok/test/bartok434.mid\n",
      "Parsing small_dataset/bartok/test/bartok436.mid\n"
     ]
    }
   ],
   "source": [
    "from music21 import converter, instrument, note, chord\n",
    "import glob, pickle\n",
    "\n",
    "all_notes = []\n",
    "for artist in artists_list:\n",
    "    artist_path = os.path.join('small_dataset', artist)\n",
    "    for t in ['train', 'test']:\n",
    "        files_path = os.path.join(artist_path, t)\n",
    "        notes = []\n",
    "        for file in glob.glob(files_path+'/*.mid'):\n",
    "            midi = converter.parse(file)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            notes_to_parse = None\n",
    "\n",
    "            try: # file has instrument parts\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                notes_to_parse = s2.parts[0].recurse() \n",
    "            except: # file has notes in a flat structure\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        notes_path = os.path.join(artist_path, '{}_notes'.format(t))\n",
    "        with open(notes_path, 'wb') as filepath:\n",
    "            pickle.dump(notes, filepath)\n",
    "        all_notes += notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save note IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique notes 525\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sorted_notes = sorted(set(all_notes))\n",
    "notes_dict = {note:i for i,note in enumerate(sorted_notes)}\n",
    "json.dump(notes_dict, open(os.path.join(out_dir, 'note_ids.json'), 'w'))\n",
    "\n",
    "print('No. of unique notes', len(sorted_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2669, device='cuda:0')\n",
      "starting\n",
      "Val loss 6.264480113983154\n",
      "Train loss 4.388252258300781, Val loss 4.2965168952941895\n",
      "Train loss 4.248142242431641, Val loss 4.275274276733398\n",
      "Train loss 4.2382941246032715, Val loss 4.25875997543335\n",
      "Train loss 4.230668544769287, Val loss 4.264562606811523\n",
      "Train loss 4.226040363311768, Val loss 4.27239990234375\n",
      "Train loss 4.2188897132873535, Val loss 4.23147439956665\n",
      "Train loss 4.193641662597656, Val loss 4.228265762329102\n",
      "Train loss 4.223145484924316, Val loss 4.251105785369873\n",
      "Train loss 4.205316543579102, Val loss 4.216416835784912\n",
      "Train loss 4.199973106384277, Val loss 4.24558687210083\n",
      "Train loss 4.139345645904541, Val loss 4.167675495147705\n",
      "Train loss 4.104101657867432, Val loss 4.15146541595459\n",
      "Train loss 4.085698127746582, Val loss 4.1471052169799805\n",
      "Train loss 4.077186584472656, Val loss 4.173401355743408\n",
      "Train loss 4.06749963760376, Val loss 4.1487884521484375\n",
      "Train loss 4.05368709564209, Val loss 4.166123390197754\n",
      "Train loss 4.04655647277832, Val loss 4.155883312225342\n",
      "Train loss 4.023285388946533, Val loss 4.1785197257995605\n",
      "Train loss 4.004796028137207, Val loss 4.170890808105469\n",
      "Train loss 3.976393222808838, Val loss 4.177340507507324\n",
      "Train loss 3.953418016433716, Val loss 4.1785888671875\n",
      "Train loss 3.917166233062744, Val loss 4.188321113586426\n",
      "Train loss 3.876793622970581, Val loss 4.189973831176758\n",
      "Train loss 3.8274242877960205, Val loss 4.210719108581543\n",
      "Train loss 3.7825429439544678, Val loss 4.227015972137451\n",
      "Train loss 3.7412145137786865, Val loss 4.259634017944336\n",
      "Train loss 3.662091016769409, Val loss 4.276220798492432\n",
      "Train loss 3.603900909423828, Val loss 4.297567844390869\n",
      "Train loss 3.5221002101898193, Val loss 4.313510417938232\n",
      "Train loss 3.426403045654297, Val loss 4.363231182098389\n",
      "Train loss 3.347320318222046, Val loss 4.423843860626221\n",
      "Train loss 3.2552006244659424, Val loss 4.434953689575195\n",
      "Train loss 3.144481658935547, Val loss 4.472342491149902\n",
      "Train loss 3.034057855606079, Val loss 4.514882564544678\n",
      "Train loss 2.906430959701538, Val loss 4.55930757522583\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# dataset for classification\n",
    "class predictionDataset(Dataset):\n",
    "    def __init__(self, notes, notes_dict, sequence_length=100):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_notes = len(notes_dict)\n",
    "        for i in range(0, len(notes) - sequence_length, 1):\n",
    "            sequence_in = notes[i:i + sequence_length]\n",
    "            sequence_out = notes[i + sequence_length]\n",
    "            self.inputs.append([notes_dict[char] for char in sequence_in])\n",
    "            self.outputs.append(notes_dict[sequence_out])\n",
    "        self.inputs = np.array(self.inputs).reshape(-1, self.sequence_length,1)/self.num_notes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return torch.FloatTensor(self.inputs[idx]),\\\n",
    "               torch.tensor(self.outputs[idx], dtype=torch.long)\n",
    "    \n",
    "data_dir = 'small_dataset'\n",
    "notes_dict = json.load(open(os.path.join(data_dir, 'note_ids.json'), 'rb'))\n",
    "num_notes = len(notes_dict)\n",
    "artist = 'bach'\n",
    "notes_path = os.path.join(data_dir, artist, 'train_notes')\n",
    "with open(notes_path, 'rb') as file:\n",
    "    notes = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "class melodyNet(nn.Module):\n",
    "    def __init__(self, obs_dim=1, hidden_dim=512, out_dim=525, layers=1):\n",
    "        super(melodyNet, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(self.obs_dim, hidden_dim, layers) \n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        # reshape,feed to lstm\n",
    "        out = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        out, _ = self.lstm(out)                       # [seq_len, batch_size, hidden_dim]            \n",
    "        out = out[-1]         # [batch_size, hidden_dim]\n",
    "        out = self.fc(out) # batch_size, out_dim\n",
    "        return(out)\n",
    "\n",
    "\n",
    "val_frac = 0.8\n",
    "batch_size = 20\n",
    "device = 'cuda:0'\n",
    "myDataset = predictionDataset(notes, notes_dict)\n",
    "dataset_size = len(myDataset)\n",
    "val_size = int(val_frac*dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(myDataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "\n",
    "myNet = melodyNet()\n",
    "\n",
    "# optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=learning_rate)\n",
    "lr_func = lambda e: 0.99**e\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "epochs = 100\n",
    "\n",
    "myNet = myNet.to(device)\n",
    "for x,y in train_loader:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = myNet(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        print(loss)\n",
    "    break\n",
    "    \n",
    "    \n",
    "# train\n",
    "print('starting')\n",
    "myNet = myNet.to(device)\n",
    "t_start = time.time()\n",
    "# tensorboard\n",
    "\n",
    "writer = SummaryWriter('logs/generation_{}_1lstm_1_fc_lr_{}_2'.format(artist, learning_rate))\n",
    "\n",
    "def validate():\n",
    "    val_loss_epoch, c = 0, 0\n",
    "    for X,Y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = myNet(X.to(device))\n",
    "            loss = criterion(outputs.to(device), Y.to(device))\n",
    "            val_loss_epoch += loss.data\n",
    "            c += 1\n",
    "    val_loss_epoch /= c\n",
    "    return val_loss_epoch\n",
    "    \n",
    "val_loss = validate()\n",
    "print('Val loss {}'.format(val_loss))\n",
    "writer.add_scalar('validation loss', val_loss, 0)\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss_epoch, c = 0, 0\n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = myNet(X.to(device))\n",
    "        loss = criterion(outputs.to(device), Y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.data\n",
    "        c += 1\n",
    "    loss_epoch /= c\n",
    "    writer.add_scalar('training loss', loss_epoch, e+1)\n",
    "#     scheduler.step()\n",
    "    val_loss_epoch = validate()\n",
    "    writer.add_scalar('validation loss', val_loss_epoch, e+1)\n",
    "    print('Train loss {}, Val loss {}'.format(loss_epoch, val_loss_epoch))\n",
    "t_end = time.time()\n",
    "print('time taken {}'.format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize & playback 10 sec clip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(ary):\n",
    "    part = np.repeat(ary, 500, axis=0)\n",
    "    plt.plot(range(part.shape[0]), np.multiply(np.where(part>0, 1, 0), range(1, 89)), marker='.', markersize=1, linestyle='')\n",
    "    plt.title(\"Midi clip\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data = np.load('./preprocessed_separate/bach/100.npz')\n",
    "part = data['clip']\n",
    "visualize(part)\n",
    "print('Artist idx', data['artist_idx'])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def arry2mid(ary, tempo=500000):\n",
    "    # get the difference\n",
    "    ary = np.repeat(ary, 500, axis=0)\n",
    "    new_ary = np.concatenate([np.array([[0] * 88]), np.array(ary)], axis=0)\n",
    "    changes = new_ary[1:] - new_ary[:-1]\n",
    "    # create a midi file with an empty track\n",
    "    mid_new = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid_new.tracks.append(track)\n",
    "    track.append(mido.MetaMessage('set_tempo', tempo=tempo, time=0))\n",
    "    # add difference in the empty track\n",
    "    last_time = 0\n",
    "    for ch in changes:\n",
    "        if set(ch) == {0}:  # no change\n",
    "            last_time += 1\n",
    "        else:\n",
    "            on_notes = np.where(ch > 0)[0]\n",
    "            on_notes_vol = ch[on_notes]\n",
    "            off_notes = np.where(ch < 0)[0]\n",
    "            first_ = True\n",
    "            for n, v in zip(on_notes, on_notes_vol):\n",
    "                new_time = last_time if first_ else 0\n",
    "                track.append(mido.Message('note_on', note=n + 21, velocity=v, time=new_time))\n",
    "                first_ = False\n",
    "            for n in off_notes:\n",
    "                new_time = last_time if first_ else 0\n",
    "                track.append(mido.Message('note_off', note=n + 21, velocity=0, time=new_time))\n",
    "                first_ = False\n",
    "            last_time = 0\n",
    "    return mid_new\n",
    "\n",
    "mid = arry2mid(part)\n",
    "mid.save('sample2.mid')\n",
    "\n",
    "\n",
    "from music21 import midi\n",
    "print('1')\n",
    "mf = midi.MidiFile()\n",
    "print('2')\n",
    "mf.open('sample2.mid') # path='abc.midi'\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "s.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for classification\n",
    "class classifyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.num_files = len(os.listdir(os.path.join(root_dir, 'data')))\n",
    "        self.files = [os.path.join(root_dir, 'data', '{}.npz'.format(i)) for i in range(self.num_files)]\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "#         print(data['clip'].shape)\n",
    "        return torch.FloatTensor(data['clip']),\\\n",
    "               torch.ones(data['clip'].shape[0], dtype=torch.long)* torch.tensor(data['artist_idx'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('started')\n",
    "class artistClassifyNet(nn.Module):\n",
    "    def __init__(self, num_artists=10, obs_dim=88, hidden_dim=100):\n",
    "        super(artistClassifyNet, self).__init__()\n",
    "        self.num_artists = num_artists\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(self.obs_dim, hidden_dim)           # Input dim is 3, output dim is 3\n",
    "        self.fc = nn.Linear(hidden_dim, num_artists)\n",
    "    \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        out = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        out, _ = self.lstm(out)                          # initialize the hidden states with some data\n",
    "        \n",
    "        # reshape and pass through fcn\n",
    "        out = out.transpose(0,1).contiguous().view(-1,self.hidden_dim)    # [(batch_size)*(seqLen-initSteps)) X hiddenDim]\n",
    "        out = self.fc(out)                                              # [(batch_size*seq_len)x1]\n",
    "        out = out.view(batchSize, seqLen, self.num_artists)\n",
    "        return(out)\n",
    "\n",
    "def loss_fn(outputs,labels,criterion):\n",
    "    _,_,outDim = outputs.shape\n",
    "    loss = criterion(outputs.contiguous().view(-1,outDim), labels.contiguous().view(-1))\n",
    "    return(loss)\n",
    "    \n",
    "val_frac = 0.8\n",
    "batch_size = 10\n",
    "device = 'cuda:0'\n",
    "class_dataset = classifyDataset('./preprocessed_dataset')\n",
    "dataset_size = len(class_dataset)\n",
    "val_size = int(val_frac*dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(class_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "myNet = artistClassifyNet()\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    with torch.no_grad():\n",
    "        out = myNet(x)\n",
    "        loss = loss_fn(out, y, criterion)\n",
    "        print(loss)\n",
    "    break\n",
    "    \n",
    "# optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=learning_rate)\n",
    "# lr_func = lambda e: 0.99**e\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "epochs = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "print('starting')\n",
    "myNet = myNet.to(device)\n",
    "t_start = time.time()\n",
    "# tensorboard\n",
    "\n",
    "writer = SummaryWriter('logs/classification_1lstm_1fc_lr_{}_3'.format(learning_rate))\n",
    "\n",
    "def validate():\n",
    "    val_loss_epoch, c = 0, 0\n",
    "    correct, total = 0, 0\n",
    "    for X,Y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = myNet(X.to(device))\n",
    "            loss = loss_fn(outputs.to(device), Y.to(device), criterion)\n",
    "            val_loss_epoch += loss.data\n",
    "            c += 1\n",
    "            total += X.shape[0]\n",
    "            correct += (torch.argmax(outputs[:,-1, :], dim=-1).cpu() == Y[:,-1]).numpy().sum()\n",
    "    val_loss_epoch /= c\n",
    "    val_accuracy = correct/total\n",
    "    return val_loss_epoch, val_accuracy\n",
    "    \n",
    "val_loss, val_acc = validate()\n",
    "print('Val loss {}, Val accuracy {}'.format(val_loss, val_acc))\n",
    "writer.add_scalar('validation accuracy', val_acc, 0)\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss_epoch, c = 0, 0\n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = myNet(X.to(device))\n",
    "        loss = loss_fn(outputs.to(device), Y.to(device), criterion)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.data\n",
    "        c += 1\n",
    "    loss_epoch /= c\n",
    "    writer.add_scalar('training loss', loss_epoch, e+1)\n",
    "#     scheduler.step()\n",
    "    val_loss_epoch, val_acc_epoch = validate()\n",
    "    writer.add_scalar('validation loss', val_loss_epoch, e+1)\n",
    "    writer.add_scalar('validation accuracy', val_acc_epoch, e+1)\n",
    "    print('Train loss {}, Val loss {}, Val accuracy {}'.format(loss_epoch, val_loss_epoch, val_acc_epoch))\n",
    "t_end = time.time()\n",
    "print('time taken {}'.format(t_end-t_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dict or entire model\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "torch.save(myNet, 'models/1_layer_lstm_classifier')\n",
    "torch.save(myNet.state_dict(), 'models/1_layer_lstm_classifier_state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation LSTM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,2))\n",
    "print(a.shape)\n",
    "a = a[None]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for classification\n",
    "class predictionDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.num_files = len(os.listdir(root_dir))\n",
    "        self.files = [os.path.join(root_dir, '{}.npz'.format(i)) for i in range(self.num_files)]\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "        return torch.FloatTensor(data['clip'][:8]/127>0.5),\\\n",
    "               torch.tensor(data['clip'][8:]/127>0.5, dtype=torch.long)\n",
    "\n",
    "\n",
    "class melodyNet(nn.Module):\n",
    "    def __init__(self, obs_dim=88, hidden_dim=100, layers=2, pred_steps=12):\n",
    "        super(melodyNet, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pred_steps = pred_steps\n",
    "        self.lstm = nn.LSTM(self.obs_dim, obs_dim, layers) \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(hidden_dim, obs_dim*2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        interm = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        interm, h = self.lstm(interm)                          # initialize the hidden states with some data\n",
    "        interm = interm[-1][None]\n",
    "        out = torch.zeros((batchSize, self.pred_steps, self.obs_dim), dtype=torch.float)\n",
    "        for i in range(self.pred_steps):\n",
    "            interm, h = self.lstm(interm, h)  # interm is [1, batch_size, hid_dim]\n",
    "            out[:, i, :] = self.softmax(interm.view(batchSize, self.obs_dim, 2))[:,:,-1]\n",
    "#             out[:, i, :] = self.fc(interm.view(-1,self.hidden_dim)).view(batchSize, 1, self.obs_dim)\n",
    "            \n",
    "        return(out)\n",
    "    \n",
    "\n",
    "class melodyNet2(nn.Module):\n",
    "    def __init__(self, obs_dim=88, hidden_dim=200, layers=5, pred_steps=12):\n",
    "        super(melodyNet2, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pred_steps = pred_steps\n",
    "        self.lstm = nn.LSTM(self.obs_dim, hidden_dim, layers) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(hidden_dim, obs_dim*2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        interm = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        interm, h = self.lstm(interm)                          # initialize the hidden states with some data\n",
    "        interm = self.fc(interm[-1])\n",
    "        interm = interm.view(1,batchSize,self.obs_dim,2)[:,:,:,-1]\n",
    "#         .view(1,batchSize,self.obs_dim, 2)[:,:,-1] # [1,batch_size, obs_dim]\n",
    "        out = torch.zeros((batchSize, self.pred_steps, self.obs_dim, 2), dtype=torch.float).to(device)\n",
    "        for i in range(self.pred_steps):\n",
    "            interm, h = self.lstm(interm, h)  # interm is [1, batch_size, hid_dim]\n",
    "            interm = self.fc(interm[-1])\n",
    "            interm = interm.view(1,batchSize,self.obs_dim,2)\n",
    "            out[:, i, :, :] += interm.view(batchSize, self.obs_dim, 2).to(device)\n",
    "#             out[:, i, :] = self.fc(interm.view(-1,self.hidden_dim)).view(batchSize, 1, self.obs_dim)\n",
    "            interm = interm[:,:,:,-1]\n",
    "            \n",
    "            \n",
    "        return(out)\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(outputs,labels,criterion):\n",
    "    _,_,_,outDim = outputs.shape\n",
    "    loss = criterion(outputs.contiguous().view(-1,outDim), labels.contiguous().view(-1))\n",
    "    return(loss)\n",
    "    \n",
    "val_frac = 0.8\n",
    "batch_size = 20\n",
    "device = 'cuda:0'\n",
    "class_dataset = predictionDataset('./preprocessed_separate/bach')\n",
    "dataset_size = len(class_dataset)\n",
    "val_size = int(val_frac*dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(class_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "myNet = melodyNet2()\n",
    "# optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([0.2,0.8]))\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=learning_rate)\n",
    "lr_func = lambda e: 0.99**e\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "epochs = 100\n",
    "\n",
    "myNet = myNet.to(device)\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    with torch.no_grad():\n",
    "        out = myNet(x.to(device))\n",
    "        loss = loss_fn(out, y.to(device), criterion.to(device))\n",
    "        print(loss)\n",
    "    break\n",
    "    \n",
    "    \n",
    "# train\n",
    "print('starting')\n",
    "myNet = myNet.to(device)\n",
    "t_start = time.time()\n",
    "# tensorboard\n",
    "\n",
    "writer = SummaryWriter('logs/generation_bach_1lstm_1_fc_lr_{}_1'.format(learning_rate))\n",
    "\n",
    "def validate():\n",
    "    val_loss_epoch, c = 0, 0\n",
    "    for X,Y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = myNet(X.to(device))\n",
    "            loss = loss_fn(outputs.to(device), Y.to(device), criterion.to(device))\n",
    "            val_loss_epoch += loss.data\n",
    "            c += 1\n",
    "    val_loss_epoch /= c\n",
    "    return val_loss_epoch\n",
    "    \n",
    "val_loss = validate()\n",
    "print('Val loss {}'.format(val_loss))\n",
    "writer.add_scalar('validation loss', val_loss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(1000):\n",
    "    loss_epoch, c = 0, 0\n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = myNet(X.to(device))\n",
    "#         print('oputputs', outputs)\n",
    "#         print('y', Y)\n",
    "#         outputs[:]=0\n",
    "        loss = loss_fn(outputs.to(device), Y.to(device), criterion)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.data\n",
    "        c += 1\n",
    "    loss_epoch /= c\n",
    "    writer.add_scalar('training loss', loss_epoch, e+1)\n",
    "    scheduler.step()\n",
    "    val_loss_epoch = validate()\n",
    "    writer.add_scalar('validation loss', val_loss_epoch, e+1)\n",
    "    print('Train loss {}, Val loss {}'.format(loss_epoch, val_loss_epoch))\n",
    "t_end = time.time()\n",
    "print('time taken {}'.format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dict or entire model\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "torch.save(myNet, 'models/bach_lstm_predictor')\n",
    "torch.save(myNet.state_dict(), 'models/bach_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare generated vs true on validation set    \n",
    "from music21 import midi\n",
    "print('here')\n",
    "def play_clip(path):\n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(path) # path='abc.midi'\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "    s = midi.translate.midiFileToStream(mf)\n",
    "    s.show('midi')\n",
    "\n",
    "\n",
    "X,Y = val_dataset[4]\n",
    "with torch.no_grad():\n",
    "    outputs = myNet(X[None].to(device)).to('cpu').numpy()[0]*127\n",
    "    outputs = np.argmax(outputs, axis=-1)\n",
    "    print('outputs', outputs.shape)\n",
    "    outputs = outputs.astype(np.uint8)\n",
    "    print('max val', np.min(outputs, axis=0))\n",
    "#     print(outputs)\n",
    "#     print(outputs.shape)\n",
    "#     outputs = np.random.choice([0,127], p=[0.99,0.01], size=(20,88))\n",
    "    x = (X.to('cpu').numpy()*127).astype(int)\n",
    "    y = (Y.to('cpu').numpy()*127).astype(int)\n",
    "    print(x.shape, y.shape, outputs.shape)\n",
    "    gt = np.concatenate((x,y), axis=0)\n",
    "    generated = np.concatenate((x,outputs), axis=0)\n",
    "    mid = arry2mid(gt)\n",
    "    mid.save('gt.mid')\n",
    "\n",
    "    mid = arry2mid(generated)\n",
    "    mid.save('generated.mid')\n",
    "\n",
    "    play_clip('gt.mid')\n",
    "    play_clip('generated.mid')\n",
    "    print(generated[-1])\n",
    "    visualize(gt)\n",
    "    visualize(generated)\n",
    "#     plt.plot(range(part.shape[0]), np.multiply(np.where(part>0, 1, 0), range(1, 89)), marker='.', markersize=1, linestyle='')\n",
    "#     plt.title(\"Generated\")\n",
    "#     plt.show()\n",
    "# for X,Y in val_loader:\n",
    "#     with torch.no_grad():\n",
    "#         \n",
    "#                 \n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "# a = np.random.random((3,3))\n",
    "# print(a)\n",
    "# print(a[[1,2],1])\n",
    "# import torch\n",
    "# torch.__version__\n",
    "int('0308')\n",
    "max([3,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randint(5, size = (5,2))\n",
    "b = torch.tensor([1,2])\n",
    "print(a,b)        \n",
    "print(torch.matmul(a,b))\n",
    "\n",
    "class musicClassifyNet(nn.Module):\n",
    "    def __init__(self, inpDim, hiddenDim, outDim, initSteps):\n",
    "        super(adversaryNet, self).__init__()\n",
    "        self.inpDim = inpDim\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.outDim = outDim\n",
    "        self.initSteps = initSteps                     \n",
    "        self.lstm1 = nn.LSTM(inpDim, hiddenDim)           # Input dim is 3, output dim is 3\n",
    "        self.lstm2 = nn.LSTM(hiddenDim, hiddenDim)\n",
    "        self.fc1 = nn.Linear(hiddenDim, outDim)\n",
    "#         self.fc2 = nn.Linear(hiddenDim, outDim)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, inp_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        out = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        initData, data = out[:self.initSteps], out[self.initSteps:]  # initialization data and actual data to generate output\n",
    "        out, h1 = self.lstm1(initData)                  # initialize the hidden states with some data\n",
    "#         _, h2 = self.lstm2(out)\n",
    "        \n",
    "        out, _ = self.lstm1(data, h1)                         # get actual output to be use for prediction\n",
    "#         out, _ = self.lstm2(out, h2)\n",
    "        \n",
    "        # reshape and pass through fcn\n",
    "        out = out.transpose(0,1).contiguous().view(-1,self.hiddenDim)    # [(batch_size)*(seqLen-initSteps)) X hiddenDim]\n",
    "        out = self.fc1(out)                                              # [(batch_size)*(seqLen-initSteps)) X outDim]\n",
    "#         out = self.fc2(out)\n",
    "        \n",
    "        # reshape and return\n",
    "        out = out.view(batchSize, seqLen-self.initSteps,self.outDim) # batch_size x (seqLen-initSteps) X outDim\n",
    "        return(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magenta",
   "language": "python",
   "name": "magenta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
