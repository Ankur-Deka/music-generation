{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mido\n",
    "import string\n",
    "from music21 import midi\n",
    "import json\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from music21 import converter, instrument, note, chord\n",
    "import glob, pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg2dict(msg):\n",
    "    result = dict()\n",
    "    if 'note_on' in msg:\n",
    "        on_ = True\n",
    "    elif 'note_off' in msg:\n",
    "        on_ = False\n",
    "    else:\n",
    "        on_ = None\n",
    "    result['time'] = int(msg[msg.rfind('time'):].split(' ')[0].split('=')[1].translate(\n",
    "        str.maketrans({a: None for a in string.punctuation})))\n",
    "\n",
    "    if on_ is not None:\n",
    "        for k in ['note', 'velocity']:\n",
    "            result[k] = int(msg[msg.rfind(k):].split(' ')[0].split('=')[1].translate(\n",
    "                str.maketrans({a: None for a in string.punctuation})))\n",
    "    return [result, on_]\n",
    "\n",
    "def switch_note(last_state, note, velocity, on_=True):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of this range will be ignored\n",
    "    result = [0] * 88 if last_state is None else last_state.copy()\n",
    "    if 21 <= note <= 108:\n",
    "        result[note-21] = velocity if on_ else 0\n",
    "    return result\n",
    "\n",
    "def get_new_state(new_msg, last_state):\n",
    "    new_msg, on_ = msg2dict(str(new_msg))\n",
    "    new_state = switch_note(last_state, note=new_msg['note'], velocity=new_msg['velocity'], on_=on_)\\\n",
    "                if on_ is not None else last_state\n",
    "    return [new_state, new_msg['time']]\n",
    "\n",
    "def track2seq(track):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of the id range will be ignored\n",
    "    result = []\n",
    "    last_state, last_time = get_new_state(str(track[0]), [0]*88)\n",
    "    for i in range(1, len(track)):\n",
    "        new_state, new_time = get_new_state(track[i], last_state)\n",
    "        if new_time > 0:\n",
    "            result += [last_state]*new_time\n",
    "        last_state, last_time = new_state, new_time\n",
    "    return result\n",
    "\n",
    "def mid2arry(mid, min_msg_pct=0.1):\n",
    "    tracks_len = [len(tr) for tr in mid.tracks]\n",
    "    min_n_msg = max(tracks_len) * min_msg_pct\n",
    "    # convert each track to nested list\n",
    "    all_arys = []\n",
    "    for i in range(len(mid.tracks)):\n",
    "        if len(mid.tracks[i]) > min_n_msg:\n",
    "            ary_i = track2seq(mid.tracks[i])\n",
    "            all_arys.append(ary_i)\n",
    "    # make all nested list the same length\n",
    "    max_len = max([len(ary) for ary in all_arys])\n",
    "    for i in range(len(all_arys)):\n",
    "        if len(all_arys[i]) < max_len:\n",
    "            all_arys[i] += [[0] * 88] * (max_len - len(all_arys[i])) # adding 0's at the end\n",
    "                            \n",
    "    final_arr = np.array(all_arys, dtype=np.int8)\n",
    "    final_arr = final_arr.max(axis=0)\n",
    "    # trim: remove consecutive 0s in the beginning and at the end\n",
    "    sums = final_arr.sum(axis=1)\n",
    "    ends = np.where(sums > 0)[0]\n",
    "    final_arr = final_arr[min(ends): max(ends)]\n",
    "    return final_arr\n",
    "\n",
    "\n",
    "# extracts overlapping 10sec clips from single midi_file\n",
    "def extract_clips(file, idx, store_path, counter = 0, ticks = 10000, overlap=False):\n",
    "    # load the file\n",
    "    mid = mido.MidiFile(file)\n",
    "    \n",
    "    # extract array\n",
    "    array = mid2arry(mid)\n",
    "    array2 = array[ticks//2:] # to get 50% overlap\n",
    "    \n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    \n",
    "    i = counter\n",
    "    arrays = [array,array2] if overlap else [array] \n",
    "    for arr in arrays:\n",
    "        n = (arr.shape[0]//ticks)*ticks\n",
    "        clips = np.array_split(arr, np.arange(ticks, n, ticks))\n",
    "        for c in clips[:-1]:\n",
    "            np.savez_compressed('{}/{}'.format(store_path,i), clip=c[::500], artist_idx=idx)\n",
    "            i += 1\n",
    "    return i\n",
    "    \n",
    "def preprocess(data_dir, store_path, min_clips=100):\n",
    "    print('Preprocessing data')\n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    artists_list  = np.sort(os.listdir(data_dir))\n",
    "    data_path = os.path.join(store_path, 'data')\n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'w') as f:\n",
    "        f.write(str(artists_list))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.arange(artists_list.shape[0])))\n",
    "    counter = 0\n",
    "    counts_string = '\\n'\n",
    "    for idx,artist in enumerate(artists_list):\n",
    "        path = os.path.join(data_dir, artist)\n",
    "        files = os.listdir(path)\n",
    "        midi_files = []\n",
    "        for f in files:\n",
    "            if f.endswith('mid'):\n",
    "                midi_files.append(f)\n",
    "        midi_files.sort()\n",
    "        prev_counter = counter\n",
    "        for file in midi_files:\n",
    "            counter = extract_clips(os.path.join(path, file),idx, data_path, counter)\n",
    "            if counter-prev_counter>=min_clips:\n",
    "                break\n",
    "        counts_string += '{}: {}\\n'.format(artist, counter-prev_counter)\n",
    "        print(artist, counter-prev_counter)\n",
    "    \n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'a') as f:\n",
    "        f.write(counts_string)\n",
    "\n",
    "# stores files from each artist separately (useful for seq2seq, generation)\n",
    "def preprocess_separate(data_dir, store_path, min_clips=1000):\n",
    "    print('Preprocessing data')\n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    artists_list  = np.sort(os.listdir(data_dir))\n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'w') as f:\n",
    "        f.write(str(artists_list))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.arange(artists_list.shape[0])))\n",
    "    counts_string = '\\n'\n",
    "    for idx,artist in enumerate(artists_list):\n",
    "        path = os.path.join(data_dir, artist)\n",
    "        files = os.listdir(path)\n",
    "        midi_files = []\n",
    "        for f in files:\n",
    "            if f.endswith('mid'):\n",
    "                midi_files.append(f)\n",
    "        midi_files.sort()\n",
    "        counter = 0\n",
    "        for file in midi_files:\n",
    "            counter = extract_clips(os.path.join(path, file),idx, os.path.join(store_path, artist), counter)\n",
    "            if counter>=min_clips:\n",
    "                break\n",
    "        counts_string += '{}: {}\\n'.format(artist, counter)\n",
    "        print(artist, counter)\n",
    "        break\n",
    "    \n",
    "    with open(os.path.join(store_path, 'readme.txt'), 'a') as f:\n",
    "        f.write(counts_string)\n",
    "\n",
    "# preprocess('./dataset', './preprocessed_dataset')\n",
    "preprocess_separate('./dataset', './preprocessed_separate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small dataset\n",
    "## Extract notes of bach and bartok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# split into test and train\n",
    "in_dir = 'dataset'\n",
    "out_dir = 'small_dataset'\n",
    "test_frac = 0.2\n",
    "# first split the dataset\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "# artists_list  = np.sort(os.listdir(data_dir))\n",
    "artists_list = ['bach', 'bartok']\n",
    "\n",
    "def create_copies(in_path, out_path, files):\n",
    "    for f in files:\n",
    "        shutil.copyfile(os.path.join(in_path, f), os.path.join(out_path, f))\n",
    "for artist in artists_list:\n",
    "    files = os.listdir(os.path.join(in_dir, artist))\n",
    "    files.sort()\n",
    "    n = len(files)\n",
    "    n_test = int(test_frac*n)\n",
    "    n_train = n - n_test\n",
    "    \n",
    "    if not os.path.exists(os.path.join(out_dir, artist,'train')):\n",
    "        os.makedirs(os.path.join(out_dir, artist, 'train'))\n",
    "        os.makedirs(os.path.join(out_dir, artist, 'test'))\n",
    "    \n",
    "    train_files = files[:n_train]\n",
    "    test_files = files[n_train:]\n",
    "    \n",
    "    create_copies(os.path.join(in_dir, artist), os.path.join(out_dir, artist, 'train'), train_files)\n",
    "    create_copies(os.path.join(in_dir, artist), os.path.join(out_dir, artist, 'test'), test_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract notes for each and get total unique notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_notes = []\n",
    "for artist in artists_list:\n",
    "    artist_path = os.path.join('small_dataset', artist)\n",
    "    \n",
    "    for t in ['train', 'test']:\n",
    "        artist_notes = []\n",
    "        files_path = os.path.join(artist_path, t)\n",
    "        for file in glob.glob(files_path+'/*.mid'):\n",
    "            song_notes = []\n",
    "            midi = converter.parse(file)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            notes_to_parse = None\n",
    "\n",
    "            try: # file has instrument parts\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                x = [len(v.recurse()) for v in s2.parts]\n",
    "                idx = argmax(x)\n",
    "                notes_to_parse = s2.parts[idx].recurse() \n",
    "            except: # file has notes in a flat structure\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    song_notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    song_notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "            artist_notes.append(song_notes)\n",
    "            \n",
    "        notes_path = os.path.join(artist_path, '{}_notes'.format(t))\n",
    "        with open(notes_path, 'wb') as filepath:\n",
    "            pickle.dump(artist_notes, filepath)\n",
    "        \n",
    "        for song_notes in artist_notes:\n",
    "            all_notes += song_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save note IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_notes = sorted(set(all_notes))\n",
    "notes_dict = {note:i for i,note in enumerate(sorted_notes)}\n",
    "json.dump(notes_dict, open(os.path.join(out_dir, 'note_ids.json'), 'w'))\n",
    "\n",
    "print('No. of unique notes', len(sorted_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Val loss 6.368923187255859\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# dataset for classification\n",
    "class predictionDataset(Dataset):\n",
    "    def __init__(self, songs, notes_dict, sequence_length=100):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_notes = len(notes_dict)\n",
    "        for notes in songs:\n",
    "            for i in range(0, len(notes) - sequence_length, 1):\n",
    "                sequence_in = notes[i:i + sequence_length]\n",
    "                sequence_out = notes[i + sequence_length]\n",
    "                self.inputs.append([notes_dict[char] for char in sequence_in])\n",
    "                self.outputs.append(notes_dict[sequence_out])\n",
    "        self.inputs = np.array(self.inputs).reshape(-1, self.sequence_length,1)/self.num_notes\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return torch.FloatTensor(self.inputs[idx]),\\\n",
    "               torch.tensor(self.outputs[idx], dtype=torch.long)\n",
    "    \n",
    "data_dir = 'small_dataset'\n",
    "notes_dict = json.load(open(os.path.join(data_dir, 'note_ids.json'), 'rb'))\n",
    "num_notes = len(notes_dict)\n",
    "artist = 'bach'\n",
    "notes_path = os.path.join(data_dir, artist, 'train_notes')\n",
    "with open(notes_path, 'rb') as file:\n",
    "    songs = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "class melodyNet(nn.Module):\n",
    "    def __init__(self, out_dim, obs_dim=1, hidden_dim=512, layers=1):\n",
    "        super(melodyNet, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(self.obs_dim, hidden_dim, layers) \n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,h=None):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        # reshape,feed to lstm\n",
    "        out = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        if h is None:\n",
    "              out, h = self.lstm(out)                       # [seq_len, batch_size, hidden_dim]            \n",
    "        else:\n",
    "              out, h = self.lstm(out, h)\n",
    "        out = out[-1]         # [batch_size, hidden_dim]\n",
    "        out = self.dp(self.bn1(self.fc1(out))) # batch_size, out_dim\n",
    "        out = self.fc2(out)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "\n",
    "val_frac = 0.8\n",
    "batch_size = 20\n",
    "device = 'cuda:0'\n",
    "myDataset = predictionDataset(songs, notes_dict)\n",
    "dataset_size = len(myDataset)\n",
    "val_size = int(val_frac*dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(myDataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "\n",
    "myNet = melodyNet(num_notes)\n",
    "\n",
    "# optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=learning_rate)\n",
    "lr_func = lambda e: 0.99**e\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "epochs = 100\n",
    "\n",
    "myNet = myNet.to(device)\n",
    "# for x,y in train_loader:\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         out = myNet(x.to(device))\n",
    "#         loss = criterion(out, y.to(device))\n",
    "#         print(loss)\n",
    "#     break\n",
    "    \n",
    "    \n",
    "# train\n",
    "print('starting')\n",
    "myNet = myNet.to(device)\n",
    "t_start = time.time()\n",
    "# tensorboard\n",
    "\n",
    "writer = SummaryWriter('logs/generation_{}_1lstm_1_fc_lr_{}_2'.format(artist, learning_rate))\n",
    "\n",
    "def validate():\n",
    "    val_loss_epoch, c = 0, 0\n",
    "    myNet.eval()\n",
    "    for X,Y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = myNet(X.to(device))\n",
    "            loss = criterion(outputs.to(device), Y.to(device))\n",
    "            val_loss_epoch += loss.data\n",
    "            c += 1\n",
    "    myNet.train()\n",
    "    val_loss_epoch /= c\n",
    "    return val_loss_epoch\n",
    "    \n",
    "val_loss = validate()\n",
    "print('Val loss {}'.format(val_loss))\n",
    "writer.add_scalar('validation loss', val_loss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n"
     ]
    }
   ],
   "source": [
    "print(num_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Train loss 4.166576385498047, Val loss 4.75467586517334\n",
      "Epoch  1\n",
      "Train loss 4.150659084320068, Val loss 16.248369216918945\n",
      "Epoch  2\n",
      "Train loss 4.145793914794922, Val loss 9.664836883544922\n",
      "Epoch  3\n",
      "Train loss 4.143385410308838, Val loss 7.904368877410889\n",
      "Epoch  4\n",
      "Train loss 4.13670015335083, Val loss 10.290194511413574\n",
      "Epoch  5\n",
      "Train loss 4.132782936096191, Val loss 14.2799654006958\n",
      "Epoch  6\n",
      "Train loss 4.128384113311768, Val loss 6.69474983215332\n",
      "Epoch  7\n",
      "Train loss 4.126110553741455, Val loss 6.290464878082275\n",
      "Epoch  8\n",
      "Train loss 4.126192092895508, Val loss 15.882981300354004\n",
      "Epoch  9\n",
      "Train loss 4.1223297119140625, Val loss 7.354807376861572\n",
      "Epoch  10\n",
      "Train loss 4.121462821960449, Val loss 12.899478912353516\n",
      "Epoch  11\n",
      "Train loss 4.119614601135254, Val loss 18.882434844970703\n",
      "Epoch  12\n",
      "Train loss 4.118338584899902, Val loss 27.963850021362305\n",
      "Epoch  13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-361ca1970837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mval_loss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train loss {}, Val loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3421d59cd6e2>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mval_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myNet.train()\n",
    "for e in range(epochs):\n",
    "    print('Epoch ', e)\n",
    "    loss_epoch, c = 0, 0\n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = myNet(X.to(device))\n",
    "        loss = criterion(outputs.to(device), Y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.data\n",
    "        c += 1\n",
    "    loss_epoch /= c\n",
    "    writer.add_scalar('training loss', loss_epoch, e+1)\n",
    "    scheduler.step()\n",
    "    val_loss_epoch = validate()\n",
    "    writer.add_scalar('validation loss', val_loss_epoch, e+1)\n",
    "    print('Train loss {}, Val loss {}'.format(loss_epoch, val_loss_epoch))\n",
    "    \n",
    "t_end = time.time()\n",
    "print('time taken {}'.format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "data_dir = 'small_dataset'\n",
    "notes_dict = json.load(open(os.path.join(data_dir, 'note_ids.json'), 'rb'))\n",
    "num_notes = len(notes_dict)\n",
    "artist = 'bach'\n",
    "myNet = torch.load('./logs/{}_seq2seq/best_train_model'.format(artist), map_location=device)\n",
    "myNet.eval()\n",
    "test_notes_path = os.path.join(data_dir, artist, 'test_notes')\n",
    "with open(test_notes_path, 'rb') as file:\n",
    "    test_notes = pickle.load(file)\n",
    "testDataset = predictionDataset(test_notes, notes_dict)\n",
    "print(len(test_notes))\n",
    "\n",
    "    \n",
    "def continue_seq(sequence, steps):\n",
    "    second = []\n",
    "    nxt_seq = sequence\n",
    "    h = None\n",
    "    for s in range(steps-1):\n",
    "        with torch.no_grad():\n",
    "            out, h = myNet(nxt_seq.to(device), h)\n",
    "            out = torch.argmax(out, dim=-1)\n",
    "            second.append(out.cpu().numpy())\n",
    "            out = out.view(1,1,1).type(torch.FloatTensor)/num_notes\n",
    "            nxt_seq = torch.cat((nxt_seq[:,1:,:], out), dim=1)\n",
    "    first = sequence.cpu().numpy().flatten()*(num_notes-1)\n",
    "    second = np.array(second).flatten()\n",
    "    return np.concatenate((first, second)).astype(int)\n",
    "\n",
    "# 1d numpy array\n",
    "def create_midi(sequence, notes_dict):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    reverse_dict = {v:k for k,v in notes_dict.items()}\n",
    "    notes_sequence = [reverse_dict[v] for v in sequence]\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in notes_sequence:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='output_{}.mid'.format(artist))\n",
    "    \n",
    "# inp_sequence = testDataset[1000][0].reshape(1,-1,1)\n",
    "inp_sequence = torch.randint(low=0, high=num_notes, size=(1,2,1)).type(torch.FloatTensor)/num_notes\n",
    "full_sequence = continue_seq(inp_sequence, 100)\n",
    "print(full_sequence)\n",
    "create_midi(full_sequence, notes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize & playback 10 sec clip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(ary):\n",
    "    part = np.repeat(ary, 500, axis=0)\n",
    "    plt.plot(range(part.shape[0]), np.multiply(np.where(part>0, 1, 0), range(1, 89)), marker='.', markersize=1, linestyle='')\n",
    "    plt.title(\"Midi clip\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data = np.load('./preprocessed_separate/bach/100.npz')\n",
    "part = data['clip']\n",
    "visualize(part)\n",
    "print('Artist idx', data['artist_idx'])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def arry2mid(ary, tempo=500000):\n",
    "    # get the difference\n",
    "    ary = np.repeat(ary, 500, axis=0)\n",
    "    new_ary = np.concatenate([np.array([[0] * 88]), np.array(ary)], axis=0)\n",
    "    changes = new_ary[1:] - new_ary[:-1]\n",
    "    # create a midi file with an empty track\n",
    "    mid_new = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid_new.tracks.append(track)\n",
    "    track.append(mido.MetaMessage('set_tempo', tempo=tempo, time=0))\n",
    "    # add difference in the empty track\n",
    "    last_time = 0\n",
    "    for ch in changes:\n",
    "        if set(ch) == {0}:  # no change\n",
    "            last_time += 1\n",
    "        else:\n",
    "            on_notes = np.where(ch > 0)[0]\n",
    "            on_notes_vol = ch[on_notes]\n",
    "            off_notes = np.where(ch < 0)[0]\n",
    "            first_ = True\n",
    "            for n, v in zip(on_notes, on_notes_vol):\n",
    "                new_time = last_time if first_ else 0\n",
    "                track.append(mido.Message('note_on', note=n + 21, velocity=v, time=new_time))\n",
    "                first_ = False\n",
    "            for n in off_notes:\n",
    "                new_time = last_time if first_ else 0\n",
    "                track.append(mido.Message('note_off', note=n + 21, velocity=0, time=new_time))\n",
    "                first_ = False\n",
    "            last_time = 0\n",
    "    return mid_new\n",
    "\n",
    "mid = arry2mid(part)\n",
    "mid.save('sample2.mid')\n",
    "\n",
    "\n",
    "from music21 import midi\n",
    "print('1')\n",
    "mf = midi.MidiFile()\n",
    "print('2')\n",
    "mf.open('sample2.mid') # path='abc.midi'\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "s.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for classification\n",
    "class classifyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.num_files = len(os.listdir(os.path.join(root_dir, 'data')))\n",
    "        self.files = [os.path.join(root_dir, 'data', '{}.npz'.format(i)) for i in range(self.num_files)]\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "#         print(data['clip'].shape)\n",
    "        return torch.FloatTensor(data['clip']),\\\n",
    "               torch.ones(data['clip'].shape[0], dtype=torch.long)* torch.tensor(data['artist_idx'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('started')\n",
    "class artistClassifyNet(nn.Module):\n",
    "    def __init__(self, num_artists=10, obs_dim=88, hidden_dim=100):\n",
    "        super(artistClassifyNet, self).__init__()\n",
    "        self.num_artists = num_artists\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(self.obs_dim, hidden_dim)           # Input dim is 3, output dim is 3\n",
    "        self.fc = nn.Linear(hidden_dim, num_artists)\n",
    "    \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        out = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        out, _ = self.lstm(out)                          # initialize the hidden states with some data\n",
    "        \n",
    "        # reshape and pass through fcn\n",
    "        out = out.transpose(0,1).contiguous().view(-1,self.hidden_dim)    # [(batch_size)*(seqLen-initSteps)) X hiddenDim]\n",
    "        out = self.fc(out)                                              # [(batch_size*seq_len)x1]\n",
    "        out = out.view(batchSize, seqLen, self.num_artists)\n",
    "        return(out)\n",
    "\n",
    "def loss_fn(outputs,labels,criterion):\n",
    "    _,_,outDim = outputs.shape\n",
    "    loss = criterion(outputs.contiguous().view(-1,outDim), labels.contiguous().view(-1))\n",
    "    return(loss)\n",
    "    \n",
    "val_frac = 0.8\n",
    "batch_size = 10\n",
    "device = 'cuda:0'\n",
    "class_dataset = classifyDataset('./preprocessed_dataset')\n",
    "dataset_size = len(class_dataset)\n",
    "val_size = int(val_frac*dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(class_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "myNet = artistClassifyNet()\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    with torch.no_grad():\n",
    "        out = myNet(x)\n",
    "        loss = loss_fn(out, y, criterion)\n",
    "        print(loss)\n",
    "    break\n",
    "    \n",
    "# optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=learning_rate)\n",
    "# lr_func = lambda e: 0.99**e\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "epochs = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "print('starting')\n",
    "myNet = myNet.to(device)\n",
    "t_start = time.time()\n",
    "# tensorboard\n",
    "\n",
    "writer = SummaryWriter('logs/classification_1lstm_1fc_lr_{}_3'.format(learning_rate))\n",
    "\n",
    "def validate():\n",
    "    val_loss_epoch, c = 0, 0\n",
    "    correct, total = 0, 0\n",
    "    for X,Y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = myNet(X.to(device))\n",
    "            loss = loss_fn(outputs.to(device), Y.to(device), criterion)\n",
    "            val_loss_epoch += loss.data\n",
    "            c += 1\n",
    "            total += X.shape[0]\n",
    "            correct += (torch.argmax(outputs[:,-1, :], dim=-1).cpu() == Y[:,-1]).numpy().sum()\n",
    "    val_loss_epoch /= c\n",
    "    val_accuracy = correct/total\n",
    "    return val_loss_epoch, val_accuracy\n",
    "    \n",
    "val_loss, val_acc = validate()\n",
    "print('Val loss {}, Val accuracy {}'.format(val_loss, val_acc))\n",
    "writer.add_scalar('validation accuracy', val_acc, 0)\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss_epoch, c = 0, 0\n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = myNet(X.to(device))\n",
    "        loss = loss_fn(outputs.to(device), Y.to(device), criterion)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.data\n",
    "        c += 1\n",
    "    loss_epoch /= c\n",
    "    writer.add_scalar('training loss', loss_epoch, e+1)\n",
    "#     scheduler.step()\n",
    "    val_loss_epoch, val_acc_epoch = validate()\n",
    "    writer.add_scalar('validation loss', val_loss_epoch, e+1)\n",
    "    writer.add_scalar('validation accuracy', val_acc_epoch, e+1)\n",
    "    print('Train loss {}, Val loss {}, Val accuracy {}'.format(loss_epoch, val_loss_epoch, val_acc_epoch))\n",
    "t_end = time.time()\n",
    "print('time taken {}'.format(t_end-t_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dict or entire model\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "torch.save(myNet, 'models/1_layer_lstm_classifier')\n",
    "torch.save(myNet.state_dict(), 'models/1_layer_lstm_classifier_state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation LSTM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,2))\n",
    "print(a.shape)\n",
    "a = a[None]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for classification\n",
    "class predictionDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.num_files = len(os.listdir(root_dir))\n",
    "        self.files = [os.path.join(root_dir, '{}.npz'.format(i)) for i in range(self.num_files)]\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "        return torch.FloatTensor(data['clip'][:8]/127>0.5),\\\n",
    "               torch.tensor(data['clip'][8:]/127>0.5, dtype=torch.long)\n",
    "\n",
    "\n",
    "class melodyNet(nn.Module):\n",
    "    def __init__(self, obs_dim=88, hidden_dim=100, layers=2, pred_steps=12):\n",
    "        super(melodyNet, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pred_steps = pred_steps\n",
    "        self.lstm = nn.LSTM(self.obs_dim, obs_dim, layers) \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(hidden_dim, obs_dim*2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        interm = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        interm, h = self.lstm(interm)                          # initialize the hidden states with some data\n",
    "        interm = interm[-1][None]\n",
    "        out = torch.zeros((batchSize, self.pred_steps, self.obs_dim), dtype=torch.float)\n",
    "        for i in range(self.pred_steps):\n",
    "            interm, h = self.lstm(interm, h)  # interm is [1, batch_size, hid_dim]\n",
    "            out[:, i, :] = self.softmax(interm.view(batchSize, self.obs_dim, 2))[:,:,-1]\n",
    "#             out[:, i, :] = self.fc(interm.view(-1,self.hidden_dim)).view(batchSize, 1, self.obs_dim)\n",
    "            \n",
    "        return(out)\n",
    "    \n",
    "\n",
    "class melodyNet2(nn.Module):\n",
    "    def __init__(self, obs_dim=88, hidden_dim=200, layers=5, pred_steps=12):\n",
    "        super(melodyNet2, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pred_steps = pred_steps\n",
    "        self.lstm = nn.LSTM(self.obs_dim, hidden_dim, layers) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(hidden_dim, obs_dim*2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, obs_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        interm = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        interm, h = self.lstm(interm)                          # initialize the hidden states with some data\n",
    "        interm = self.fc(interm[-1])\n",
    "        interm = interm.view(1,batchSize,self.obs_dim,2)[:,:,:,-1]\n",
    "#         .view(1,batchSize,self.obs_dim, 2)[:,:,-1] # [1,batch_size, obs_dim]\n",
    "        out = torch.zeros((batchSize, self.pred_steps, self.obs_dim, 2), dtype=torch.float).to(device)\n",
    "        for i in range(self.pred_steps):\n",
    "            interm, h = self.lstm(interm, h)  # interm is [1, batch_size, hid_dim]\n",
    "            interm = self.fc(interm[-1])\n",
    "            interm = interm.view(1,batchSize,self.obs_dim,2)\n",
    "            out[:, i, :, :] += interm.view(batchSize, self.obs_dim, 2).to(device)\n",
    "#             out[:, i, :] = self.fc(interm.view(-1,self.hidden_dim)).view(batchSize, 1, self.obs_dim)\n",
    "            interm = interm[:,:,:,-1]\n",
    "            \n",
    "            \n",
    "        return(out)\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(outputs,labels,criterion):\n",
    "    _,_,_,outDim = outputs.shape\n",
    "    loss = criterion(outputs.contiguous().view(-1,outDim), labels.contiguous().view(-1))\n",
    "    return(loss)\n",
    "    \n",
    "val_frac = 0.8\n",
    "batch_size = 20\n",
    "device = 'cuda:0'\n",
    "class_dataset = predictionDataset('./preprocessed_separate/bach')\n",
    "dataset_size = len(class_dataset)\n",
    "val_size = int(val_frac*dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(class_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "myNet = melodyNet2()\n",
    "# optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([0.2,0.8]))\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=learning_rate)\n",
    "lr_func = lambda e: 0.99**e\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "epochs = 100\n",
    "\n",
    "myNet = myNet.to(device)\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    with torch.no_grad():\n",
    "        out = myNet(x.to(device))\n",
    "        loss = loss_fn(out, y.to(device), criterion.to(device))\n",
    "        print(loss)\n",
    "    break\n",
    "    \n",
    "    \n",
    "# train\n",
    "print('starting')\n",
    "myNet = myNet.to(device)\n",
    "t_start = time.time()\n",
    "# tensorboard\n",
    "\n",
    "writer = SummaryWriter('logs/generation_bach_1lstm_1_fc_lr_{}_1'.format(learning_rate))\n",
    "\n",
    "def validate():\n",
    "    val_loss_epoch, c = 0, 0\n",
    "    for X,Y in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = myNet(X.to(device))\n",
    "            loss = loss_fn(outputs.to(device), Y.to(device), criterion.to(device))\n",
    "            val_loss_epoch += loss.data\n",
    "            c += 1\n",
    "    val_loss_epoch /= c\n",
    "    return val_loss_epoch\n",
    "    \n",
    "val_loss = validate()\n",
    "print('Val loss {}'.format(val_loss))\n",
    "writer.add_scalar('validation loss', val_loss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(1000):\n",
    "    loss_epoch, c = 0, 0\n",
    "    for X,Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = myNet(X.to(device))\n",
    "#         print('oputputs', outputs)\n",
    "#         print('y', Y)\n",
    "#         outputs[:]=0\n",
    "        loss = loss_fn(outputs.to(device), Y.to(device), criterion)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.data\n",
    "        c += 1\n",
    "    loss_epoch /= c\n",
    "    writer.add_scalar('training loss', loss_epoch, e+1)\n",
    "    scheduler.step()\n",
    "    val_loss_epoch = validate()\n",
    "    writer.add_scalar('validation loss', val_loss_epoch, e+1)\n",
    "    print('Train loss {}, Val loss {}'.format(loss_epoch, val_loss_epoch))\n",
    "t_end = time.time()\n",
    "print('time taken {}'.format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dict or entire model\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "torch.save(myNet, 'models/bach_lstm_predictor')\n",
    "torch.save(myNet.state_dict(), 'models/bach_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare generated vs true on validation set    \n",
    "from music21 import midi\n",
    "print('here')\n",
    "def play_clip(path):\n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(path) # path='abc.midi'\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "    s = midi.translate.midiFileToStream(mf)\n",
    "    s.show('midi')\n",
    "\n",
    "\n",
    "X,Y = val_dataset[4]\n",
    "with torch.no_grad():\n",
    "    outputs = myNet(X[None].to(device)).to('cpu').numpy()[0]*127\n",
    "    outputs = np.argmax(outputs, axis=-1)\n",
    "    print('outputs', outputs.shape)\n",
    "    outputs = outputs.astype(np.uint8)\n",
    "    print('max val', np.min(outputs, axis=0))\n",
    "#     print(outputs)\n",
    "#     print(outputs.shape)\n",
    "#     outputs = np.random.choice([0,127], p=[0.99,0.01], size=(20,88))\n",
    "    x = (X.to('cpu').numpy()*127).astype(int)\n",
    "    y = (Y.to('cpu').numpy()*127).astype(int)\n",
    "    print(x.shape, y.shape, outputs.shape)\n",
    "    gt = np.concatenate((x,y), axis=0)\n",
    "    generated = np.concatenate((x,outputs), axis=0)\n",
    "    mid = arry2mid(gt)\n",
    "    mid.save('gt.mid')\n",
    "\n",
    "    mid = arry2mid(generated)\n",
    "    mid.save('generated.mid')\n",
    "\n",
    "    play_clip('gt.mid')\n",
    "    play_clip('generated.mid')\n",
    "    print(generated[-1])\n",
    "    visualize(gt)\n",
    "    visualize(generated)\n",
    "#     plt.plot(range(part.shape[0]), np.multiply(np.where(part>0, 1, 0), range(1, 89)), marker='.', markersize=1, linestyle='')\n",
    "#     plt.title(\"Generated\")\n",
    "#     plt.show()\n",
    "# for X,Y in val_loader:\n",
    "#     with torch.no_grad():\n",
    "#         \n",
    "#                 \n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "# a = np.random.random((3,3))\n",
    "# print(a)\n",
    "# print(a[[1,2],1])\n",
    "# import torch\n",
    "# torch.__version__\n",
    "int('0308')\n",
    "max([3,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randint(5, size = (5,2))\n",
    "b = torch.tensor([1,2])\n",
    "print(a,b)        \n",
    "print(torch.matmul(a,b))\n",
    "\n",
    "class musicClassifyNet(nn.Module):\n",
    "    def __init__(self, inpDim, hiddenDim, outDim, initSteps):\n",
    "        super(adversaryNet, self).__init__()\n",
    "        self.inpDim = inpDim\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.outDim = outDim\n",
    "        self.initSteps = initSteps                     \n",
    "        self.lstm1 = nn.LSTM(inpDim, hiddenDim)           # Input dim is 3, output dim is 3\n",
    "        self.lstm2 = nn.LSTM(hiddenDim, hiddenDim)\n",
    "        self.fc1 = nn.Linear(hiddenDim, outDim)\n",
    "#         self.fc2 = nn.Linear(hiddenDim, outDim)\n",
    "        \n",
    "    def forward(self,x):  # x in shape [batch_size, seq_len, inp_dim]\n",
    "        batchSize, seqLen, _ = x.shape\n",
    "        \n",
    "        # reshape,feed to lstm\n",
    "        out = x.transpose(0,1)                           # reshape for lstm [seq_len, batch_size, inp_dim]\n",
    "        initData, data = out[:self.initSteps], out[self.initSteps:]  # initialization data and actual data to generate output\n",
    "        out, h1 = self.lstm1(initData)                  # initialize the hidden states with some data\n",
    "#         _, h2 = self.lstm2(out)\n",
    "        \n",
    "        out, _ = self.lstm1(data, h1)                         # get actual output to be use for prediction\n",
    "#         out, _ = self.lstm2(out, h2)\n",
    "        \n",
    "        # reshape and pass through fcn\n",
    "        out = out.transpose(0,1).contiguous().view(-1,self.hiddenDim)    # [(batch_size)*(seqLen-initSteps)) X hiddenDim]\n",
    "        out = self.fc1(out)                                              # [(batch_size)*(seqLen-initSteps)) X outDim]\n",
    "#         out = self.fc2(out)\n",
    "        \n",
    "        # reshape and return\n",
    "        out = out.view(batchSize, seqLen-self.initSteps,self.outDim) # batch_size x (seqLen-initSteps) X outDim\n",
    "        return(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magenta",
   "language": "python",
   "name": "magenta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
